{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.105091Z",
     "start_time": "2020-05-11T13:15:27.099132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Projects/doggo/notebooks\n",
      "/Users/admin/Projects/doggo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "def update_working_directory():\n",
    "    from pathlib import Path\n",
    "    p = Path(os.getcwd()).parents[0]\n",
    "    os.chdir(p)\n",
    "    print(p)\n",
    "update_working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.640632Z",
     "start_time": "2020-05-11T13:15:27.107854Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters specific to the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.645654Z",
     "start_time": "2020-05-11T13:15:27.643317Z"
    }
   },
   "outputs": [],
   "source": [
    "decimals_state = 2\n",
    "gamma = 0.95 # discount for future rewards (also called decay factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation & Action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.652604Z",
     "start_time": "2020-05-11T13:15:27.648235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030301 4\n"
     ]
    }
   ],
   "source": [
    "n_states = (10**decimals_state+1)**3 \n",
    "\n",
    "action_dict = {\n",
    "    0: 'NO ACTION',\n",
    "    1: 'WALKING',\n",
    "    2: 'EATING',\n",
    "    3: 'PLAYING'\n",
    "}\n",
    "n_actions= len(action_dict)\n",
    "\n",
    "print(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.658111Z",
     "start_time": "2020-05-11T13:15:27.654857Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_state_id(dog_state):\n",
    "    return '{:01.4f}_{:01.4f}_{:01.4f}_{}'.format(\n",
    "        dog_state['food'], dog_state['fat'], dog_state['affection'], dog_state['can_action_be_taken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.663980Z",
     "start_time": "2020-05-11T13:15:27.660218Z"
    }
   },
   "outputs": [],
   "source": [
    "def env_reset():\n",
    "    \n",
    "    dog_state = {\n",
    "        'food': 0.5,\n",
    "        'fat': 0,\n",
    "        'affection': 0.5,\n",
    "        'last_action_taken': 0,\n",
    "        'minutes_since_last_action': 0,\n",
    "        'can_action_be_taken': True\n",
    "        }\n",
    "    \n",
    "    dog_state['state_id'] = get_state_id(dog_state)\n",
    "    \n",
    "    return dog_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.673442Z",
     "start_time": "2020-05-11T13:15:27.668282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': 0.5,\n",
       " 'fat': 0,\n",
       " 'affection': 0.5,\n",
       " 'last_action_taken': 0,\n",
       " 'minutes_since_last_action': 0,\n",
       " 'can_action_be_taken': True,\n",
       " 'state_id': '0.5000_0.0000_0.5000_True'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.680860Z",
     "start_time": "2020-05-11T13:15:27.676723Z"
    }
   },
   "outputs": [],
   "source": [
    "WALKING_TIME = 15\n",
    "EATING_TIME = 1\n",
    "PLAYING_TIME = 4\n",
    "\n",
    "food_consumption_rate = 1.0 / (30 * 3600)\n",
    "affection_consumption_rate = 1.0 / (50 * 3600)\n",
    "walking_fat_converge_rate = 0.2\n",
    "walking_affection_converge_rate = 0.4\n",
    "playing_fat_converge_rate = 0.1\n",
    "playing_affection_converge_rate = 0.20\n",
    "eating_food_increase = 0.6\n",
    "eating_fat_increase = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.687043Z",
     "start_time": "2020-05-11T13:15:27.683077Z"
    }
   },
   "outputs": [],
   "source": [
    "def round_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "def round_down(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.702289Z",
     "start_time": "2020-05-11T13:15:27.689028Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_decreasing_rate(value: float, rate: float) -> float:\n",
    "    \"\"\"\n",
    "    Apply a decreasing rate to a value\n",
    "    :param value: current value\n",
    "    :param rate: per second\n",
    "    :return: updated value\n",
    "    \"\"\"\n",
    "    return value - (60 * rate)\n",
    "\n",
    "def converge(value: float, target: float, ratio: float) -> float:\n",
    "    diff: float = (target - value) * ratio\n",
    "    return value + diff\n",
    "\n",
    "\n",
    "def update_food(dog_state):\n",
    "    update_food = apply_decreasing_rate(dog_state['food'], food_consumption_rate)\n",
    "    return round_down(max(0.0, update_food), decimals=decimals_state)\n",
    "\n",
    "def update_fat(dog_state):\n",
    "    update_fat = dog_state['fat']\n",
    "    return update_fat\n",
    "\n",
    "def update_affection(dog_state):\n",
    "    update_affection = apply_decreasing_rate(dog_state['affection'], affection_consumption_rate)\n",
    "    return round_down(max(0.0, update_affection), decimals=decimals_state)\n",
    "\n",
    "\n",
    "def update_if_walking(dog_state):\n",
    "    update_fat = round_down(converge(dog_state['fat'], 0.0, walking_fat_converge_rate), decimals=decimals_state)\n",
    "    update_affection = round_up(converge(dog_state['affection'], 1.0, walking_affection_converge_rate), decimals=decimals_state)\n",
    "    return (update_fat, update_affection)\n",
    "\n",
    "def update_if_feeding(dog_state):\n",
    "    update_food = round_up(min(dog_state['food'] + eating_food_increase, 1.0), decimals=decimals_state)\n",
    "    update_fat = round_up(min(dog_state['fat'] + eating_fat_increase, 1.0), decimals=decimals_state)\n",
    "    return (update_food, update_fat)\n",
    "\n",
    "def update_if_playing(dog_state):\n",
    "    update_fat = round_down(converge(dog_state['fat'], 0.0, playing_fat_converge_rate), decimals=decimals_state)\n",
    "    update_affection = round_up(converge(dog_state['affection'], 1.0, playing_affection_converge_rate), decimals=decimals_state)\n",
    "    return (update_fat, update_affection)\n",
    "\n",
    "\n",
    "def get_happiness(dog_state):\n",
    "    happiness = min(dog_state['food'], 1.0 - dog_state['fat'], dog_state['affection'])\n",
    "    return happiness\n",
    "\n",
    "\n",
    "def update_done(dog_state):\n",
    "    happiness = get_happiness(dog_state)\n",
    "    return happiness <= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.716086Z",
     "start_time": "2020-05-11T13:15:27.704596Z"
    }
   },
   "outputs": [],
   "source": [
    "# state2, reward1, done, info = env.step(action1)\n",
    "def env_step(state1, action):\n",
    "    \n",
    "    state2 = state1.copy()\n",
    "    reward_penalty = 0\n",
    "    \n",
    "    # Affect of time\n",
    "    state2['food'] = update_food(state2)\n",
    "    state2['fat'] = update_fat(state2)\n",
    "    state2['affection'] = update_affection(state2)\n",
    "    state2['minutes_since_last_action'] += 1 \n",
    "    \n",
    "    # Applying action\n",
    "    if action != 0:\n",
    "        if state2['can_action_be_taken']:\n",
    "            reward_penalty += 0.1\n",
    "            state2['can_action_be_taken'] = False\n",
    "            state2['minutes_since_last_action'] = 0\n",
    "            state2['last_action_taken'] = action\n",
    "        else:\n",
    "            reward_penalty += 0.5\n",
    "\n",
    "    # Affect of actions\n",
    "    if (state2['last_action_taken'] == 1) & (state2['minutes_since_last_action'] == WALKING_TIME):\n",
    "        state2['fat'], state2['affection'] = update_if_walking(state2)\n",
    "        state2['can_action_be_taken'] = True\n",
    "\n",
    "    if (state2['last_action_taken'] == 2) & (state2['minutes_since_last_action'] == EATING_TIME):\n",
    "        state2['food'], state2['fat'] = update_if_feeding(state2)\n",
    "        state2['can_action_be_taken'] = True\n",
    "\n",
    "    if (state2['last_action_taken'] == 3) & (state2['minutes_since_last_action'] == PLAYING_TIME):\n",
    "        state2['fat'], state2['affection'] = update_if_playing(state2)\n",
    "        state2['can_action_be_taken'] = True\n",
    "                    \n",
    "    done = update_done(state2)\n",
    "    if done:\n",
    "        reward = -10\n",
    "    else:\n",
    "        reward = min(state2['food'], 1.0 - state2['fat'], state2['affection']) - reward_penalty\n",
    "    \n",
    "    info = None\n",
    "    \n",
    "    state2['state_id'] = get_state_id(state2)\n",
    "    \n",
    "    return (state2, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.722136Z",
     "start_time": "2020-05-11T13:15:27.718276Z"
    }
   },
   "outputs": [],
   "source": [
    "def env_render(dog_state, action, Q):\n",
    "    print(dog_state)\n",
    "    print(action)\n",
    "    print(Q[dog_state['state_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining utility functions to be used in the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.729933Z",
     "start_time": "2020-05-11T13:15:27.724640Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_Q(n_actions, init_Q_type=\"ones\"):\n",
    "    \"\"\"\n",
    "    @param n_actions the number of actions\n",
    "    @param type random, ones or zeros for the initialization\n",
    "    \"\"\"\n",
    "    if init_Q_type == \"ones\":\n",
    "        default_Q_values = np.ones(n_actions)\n",
    "    elif init_Q_type == \"random\":\n",
    "        default_Q_values = np.random.random(n_actions)\n",
    "    elif init_Q_type == \"zeros\":\n",
    "        default_Q_values = np.zeros(n_actions)\n",
    "    \n",
    "    def get_default_Q_values():\n",
    "        return default_Q_values\n",
    "\n",
    "    return defaultdict(get_default_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.735737Z",
     "start_time": "2020-05-11T13:15:27.732374Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_N(n_actions):\n",
    "    \"\"\"\n",
    "    @param n_actions the number of actions\n",
    "    \"\"\"\n",
    "    default_N_values = np.zeros(n_actions)\n",
    "    \n",
    "    def get_default_N_values():\n",
    "        return default_N_values\n",
    "\n",
    "    return defaultdict(get_default_N_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.742770Z",
     "start_time": "2020-05-11T13:15:27.738712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy generator\n",
    "rng = np.random.default_rng()  # Create a default Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.750248Z",
     "start_time": "2020-05-11T13:15:27.745657Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_best_action(Q_state):\n",
    "    winner = np.argwhere(Q_state == np.amax(Q_state))\n",
    "    winner_list = winner.flatten().tolist()\n",
    "    action = random.choice(winner_list)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.757345Z",
     "start_time": "2020-05-11T13:15:27.752311Z"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state_id, n_actions, epsilon):\n",
    "    \"\"\"\n",
    "    @param Q Q values {state, action} -> value\n",
    "    @param epsilon for exploration\n",
    "    @param n_actions number of actions\n",
    "    @param state state at time t\n",
    "    \"\"\"\n",
    "    if rng.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        action = select_best_action(Q[state_id])\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.765508Z",
     "start_time": "2020-05-11T13:15:27.759401Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_discounted_reward(t, l_rewards_episode, gamma):\n",
    "    l_discounted_reward_episode = [t_prime_reward*(gamma**t_prime) for (t_prime, t_prime_reward) in enumerate(l_rewards_episode[t:])]\n",
    "    G_k_t = sum(l_discounted_reward_episode)\n",
    "    return G_k_t\n",
    "\n",
    "def add_discounted_reward(steps_episode, gamma):\n",
    "    l_rewards_episode = [step_episode['reward'] for step_episode in steps_episode]\n",
    "    for (t, step_episode) in enumerate(steps_episode):\n",
    "        step_episode['discounted_reward'] = get_discounted_reward(t, l_rewards_episode, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update N-matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.777936Z",
     "start_time": "2020-05-11T13:15:27.773028Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_N_MC(N, step_episode, method_monte_carlo, states_already_visited=[]):\n",
    "    \n",
    "    state_id = step_episode['state']['state_id']\n",
    "    action = step_episode['action']\n",
    "    \n",
    "    previous_N_value_state = N[state_id].copy()\n",
    "    \n",
    "    if method_monte_carlo == 'first_visit':\n",
    "        if not state_id in states_already_visited:\n",
    "            new_N_value = N[state_id][action] + 1\n",
    "            previous_N_value_state[action] = new_N_value\n",
    "    \n",
    "    if method_monte_carlo == 'every_visit':\n",
    "        new_N_value = N[state_id][action] + 1\n",
    "        previous_N_value_state[action] = new_N_value\n",
    "\n",
    "    N[state_id] = previous_N_value_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Q-matrice (state-action value function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.789876Z",
     "start_time": "2020-05-11T13:15:27.783106Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_Q_MC(Q, N, step_episode, method_monte_carlo, states_already_visited=[]):\n",
    "    \n",
    "    state_id = step_episode['state']['state_id']\n",
    "    action = step_episode['action']\n",
    "    G_k_t = step_episode['discounted_reward']\n",
    "    \n",
    "    previous_Q_value_state = Q[state_id].copy()\n",
    "    \n",
    "    if method_monte_carlo == 'first_visit':\n",
    "        if not state_id in states_already_visited:\n",
    "            new_Q_value = Q[state_id][action] + (G_k_t - Q[state_id][action]) / N[state_id][action]\n",
    "            previous_Q_value_state[action] = new_Q_value\n",
    "    \n",
    "    if method_monte_carlo == 'every_visit':\n",
    "        new_Q_value = Q[state_id][action] + (G_k_t - Q[state_id][action]) / N[state_id][action]\n",
    "        previous_Q_value_state[action] = new_Q_value\n",
    "\n",
    "    Q[state_id] = previous_Q_value_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.800399Z",
     "start_time": "2020-05-11T13:15:27.792222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to learn the Q-value  - Is it temporal-difference?\n",
    "def update_Q_SARSA(Q, state1_id, action1, reward1, state2_id, action2, expected=False):\n",
    "    \n",
    "    previous_Q_value_state1 = Q[state1_id].copy()\n",
    "    \n",
    "    predict = Q[state1_id][action1] \n",
    "    \n",
    "    target = reward1 + gamma * Q[state2_id][action2] \n",
    "    if expected:\n",
    "        expected_value = np.mean(Q[state2_id])\n",
    "        target = reward1 + gamma * expected_value\n",
    "    \n",
    "    new_Q_value = Q[state1_id][action1] + alpha * (target - predict)\n",
    "    previous_Q_value_state1[action1] = new_Q_value\n",
    "    \n",
    "    Q[state1_id] = previous_Q_value_state1\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.808258Z",
     "start_time": "2020-05-11T13:15:27.802655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to learn the Q-value \n",
    "def update_Q_Qlearning(Q, state1_id, action1, reward1, state2_id, action2, expected=False):\n",
    "    \n",
    "    previous_Q_value_state1 = Q[state1_id].copy()\n",
    "    \n",
    "    predict = Q[state1_id][action1] \n",
    "    \n",
    "    target = reward1 + gamma * Q[state2_id][action2] \n",
    "    \n",
    "    new_Q_value = Q[state1_id][action1] + alpha * (target - predict)\n",
    "    previous_Q_value_state1[action1] = new_Q_value\n",
    "    \n",
    "    Q[state1_id] = previous_Q_value_state1\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Updating parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Epsilon $\\epsilon$ - Exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.814664Z",
     "start_time": "2020-05-11T13:15:27.810942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(episode, init_epsilon):\n",
    "    \n",
    "    n_epsilon = init_epsilon/(episode+1)\n",
    "    \n",
    "    return n_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Alpha $\\alpha$ - Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.820137Z",
     "start_time": "2020-05-11T13:15:27.816891Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_alpha(episode, init_alpha):\n",
    "    \n",
    "    n_alpha = init_alpha/(episode+1)\n",
    "    \n",
    "    return n_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots Reward / Steps / Happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.826219Z",
     "start_time": "2020-05-11T13:15:27.822079Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "def get_list_colors():\n",
    "    colors_list = ['r','g','b','k','darkorange','y','lime','c','m'] + list(colors._colors_full_map.values())\n",
    "    return colors_list\n",
    "\n",
    "colors = get_list_colors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.833228Z",
     "start_time": "2020-05-11T13:15:27.828668Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.842549Z",
     "start_time": "2020-05-11T13:15:27.835354Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evolution_reward(evolution_reward, method_name):\n",
    "    \n",
    "    n_moving_points = int(np.ceil(len(evolution_reward)/100))\n",
    "    y = running_mean(evolution_reward,n_moving_points)\n",
    "    x = range(len(y))\n",
    "\n",
    "    info_parameters = '{} method - {} steps \\n {} init_epsilon - {} init_alpha - {} gamma - {} nmax_steps'.format(\n",
    "        method_name, len(evolution_reward), init_epsilon, init_alpha, gamma, nmax_steps)\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.title('Evolution of Avg Reward per step per episode over time \\n (smoothed over window size {})'.format(n_moving_points))\n",
    "    plt.xlabel('Episode \\n '+ info_parameters)\n",
    "    plt.ylabel('Avg Reward per step per episode (Smoothed)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/figures/{}__reward.png'.format(method_name), format='png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.853812Z",
     "start_time": "2020-05-11T13:15:27.844626Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparison_evolution_reward(evo_training__evo_avg_reward_per_step):\n",
    "\n",
    "    n_episodes = len(list(evo_training__evo_avg_reward_per_step.values())[0])\n",
    "    info_parameters = 'All methods - {} episodes \\n {} init_epsilon - {} init_alpha - {} gamma - {} nmax_steps'.format(\n",
    "        n_episodes, init_epsilon, init_alpha, gamma, nmax_steps)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    cnt=0\n",
    "    for method in list(evo_training__evo_avg_reward_per_step.keys()):\n",
    "\n",
    "        n_moving_points = int(np.ceil(len(evo_training__evo_avg_reward_per_step[method])/100))\n",
    "        y = running_mean(evo_training__evo_avg_reward_per_step[method], n_moving_points)\n",
    "        x = range(len(y))\n",
    "\n",
    "        plt.plot(\n",
    "            x\n",
    "            , y\n",
    "            , label=method\n",
    "            , marker='', color=colors[cnt], linewidth=1, alpha=0.75\n",
    "        )\n",
    "        cnt += 1\n",
    "\n",
    "    plt.title('Evolution of Avg Reward per step per episode over time \\n (smoothed over window size {})'.format(n_moving_points))\n",
    "    plt.xlabel('Episode \\n '+ info_parameters)\n",
    "    plt.ylabel('Avg Reward per step \\n per episode (Smoothed)')\n",
    "    plt.legend(bbox_to_anchor=(0.5,-0.10), loc=\"lower center\", \n",
    "                bbox_transform=fig.transFigure, ncol=4, fancybox=True, shadow=True, borderpad=1)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/figures/Comparison__reward.png', format='png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.863165Z",
     "start_time": "2020-05-11T13:15:27.855652Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evolution_steps(evolution_steps, method_name):\n",
    "    \n",
    "    n_moving_points = int(np.ceil(len(evolution_steps)/100))\n",
    "    y = running_mean(evolution_steps,n_moving_points)\n",
    "    x = range(len(y))\n",
    "\n",
    "    info_parameters = '{} method - {} steps \\n {} init_epsilon - {} init_alpha - {} gamma - {} nmax_steps'.format(\n",
    "        method_name, len(evolution_steps), init_epsilon, init_alpha, gamma, nmax_steps)\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.title('Episode Length over time \\n (smoothed over window size {})'.format(n_moving_points))\n",
    "    plt.axhline(nmax_steps, color = 'r')\n",
    "    plt.axhline(0, color = 'b')\n",
    "    plt.ylim([-10, nmax_steps*1.05])\n",
    "    plt.xlabel('Episode \\n '+ info_parameters)\n",
    "    plt.ylabel('Episode Length (Smoothed)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/figures/{}__steps.png'.format(method_name), format='png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.874622Z",
     "start_time": "2020-05-11T13:15:27.865307Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparison_evolution_steps(evo_training__evo_n_steps):\n",
    "\n",
    "    n_episodes = len(list(evo_training__evo_n_steps.values())[0])\n",
    "    info_parameters = 'All methods - {} episodes \\n {} init_epsilon - {} init_alpha - {} gamma - {} nmax_steps'.format(\n",
    "        n_episodes, init_epsilon, init_alpha, gamma, nmax_steps)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    cnt=0\n",
    "    for method in list(evo_training__evo_n_steps.keys()):\n",
    "\n",
    "        n_moving_points = int(np.ceil(len(evo_training__evo_n_steps[method])/100))\n",
    "        y = running_mean(evo_training__evo_n_steps[method], n_moving_points)\n",
    "        x = range(len(y))\n",
    "\n",
    "        plt.plot(\n",
    "            x, y, label=method\n",
    "            , marker='', color=colors[cnt], linewidth=1, alpha=0.75\n",
    "        )\n",
    "        cnt += 1\n",
    "\n",
    "    plt.title('Episode Length over time \\n (smoothed over window size {})'.format(n_moving_points))\n",
    "    plt.axhline(nmax_steps, color = 'r')\n",
    "    plt.axhline(0, color = 'b')\n",
    "    plt.ylim([-10, nmax_steps*1.05])\n",
    "    plt.xlabel('Episode \\n '+ info_parameters)\n",
    "    plt.ylabel('Episode Length (Smoothed)')\n",
    "    plt.legend(bbox_to_anchor=(0.5,-0.10), loc=\"lower center\", \n",
    "            bbox_transform=fig.transFigure, ncol=4, fancybox=True, shadow=True, borderpad=1)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/figures/Comparison__steps.png', format='png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.883251Z",
     "start_time": "2020-05-11T13:15:27.876826Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evolution_happiness(evolution_happiness_all, method_name):\n",
    "    \n",
    "    n_moving_points = int(np.ceil(len(evolution_happiness_all)/100))\n",
    "    y = running_mean(evolution_happiness_all,n_moving_points)\n",
    "    x = range(len(y))\n",
    "\n",
    "    info_parameters = '{} method - {} steps \\n {} init_epsilon - {} init_alpha - {} gamma - {} nmax_steps'.format(\n",
    "        method_name, len(evolution_happiness_all), init_epsilon, init_alpha, gamma, nmax_steps)\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.title('Happiness over time \\n (smoothed over window size {})'.format(n_moving_points))\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Episode \\n '+ info_parameters)\n",
    "    plt.ylabel('Happiness (Smoothed)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/figures/{}__happiness.png'.format(method_name), format='png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.894860Z",
     "start_time": "2020-05-11T13:15:27.885480Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparison_evolution_happiness(evo_training__evo_avg_happiness):\n",
    "\n",
    "    n_episodes = len(list(evo_training__evo_avg_happiness.values())[0])\n",
    "    info_parameters = 'All methods - {} episodes \\n {} init_epsilon - {} init_alpha - {} gamma - {} nmax_steps'.format(\n",
    "        n_episodes, init_epsilon, init_alpha, gamma, nmax_steps)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    cnt=0\n",
    "    for method in list(evo_training__evo_avg_happiness.keys()):\n",
    "\n",
    "        n_moving_points = int(np.ceil(len(evo_training__evo_avg_happiness[method])/100))\n",
    "        y = running_mean(evo_training__evo_avg_happiness[method], n_moving_points)\n",
    "        x = range(len(y))\n",
    "\n",
    "        plt.plot(\n",
    "            x, y, label=method\n",
    "            , marker='', color=colors[cnt], linewidth=1, alpha=0.75\n",
    "        )\n",
    "        cnt += 1\n",
    "\n",
    "    plt.title('Happiness over time \\n (smoothed over window size {})'.format(n_moving_points))\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Episode \\n '+ info_parameters)\n",
    "    plt.ylabel('Happiness (Smoothed)')\n",
    "    plt.legend(bbox_to_anchor=(0.5,-0.10), loc=\"lower center\", \n",
    "            bbox_transform=fig.transFigure, ncol=4, fancybox=True, shadow=True, borderpad=1)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/figures/Comparison__happiness.png', format='png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.900038Z",
     "start_time": "2020-05-11T13:15:27.896981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the different parameters\n",
    "init_epsilon = 1 # trade-off exploration/exploitation\n",
    "init_alpha = 0.5 # learning rate\n",
    "init_Q_type = 'ones'\n",
    "\n",
    "# Episodes\n",
    "n_episodes = 10000\n",
    "nmax_steps = 60*24 # maximum steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the learning agent - Monte-Carlo - every visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.904776Z",
     "start_time": "2020-05-11T13:15:27.901792Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'MC'\n",
    "method_monte_carlo = 'every_visit' # every_visit or first_visit\n",
    "\n",
    "method_name = method + '_' + method_monte_carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.909987Z",
     "start_time": "2020-05-11T13:15:27.906834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the Q-matrix \n",
    "Q = init_Q(n_actions, init_Q_type)\n",
    "N = init_N(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.915833Z",
     "start_time": "2020-05-11T13:15:27.912401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "(render_episode, render_training) = (False, False)\n",
    "n_episodes_plot = int(np.ceil(n_episodes/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:15:27.921264Z",
     "start_time": "2020-05-11T13:15:27.917930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the reward\n",
    "evo_training = {\n",
    "    'evo_avg_reward_per_step': []\n",
    "    , 'evo_n_steps': []\n",
    "    , 'evo_avg_happiness': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.794563Z",
     "start_time": "2020-05-11T13:15:27.923464Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1537/10000 [06:38<36:35,  3.86it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2284878cc5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Add discounted reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0madd_discounted_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a9b8c34e105d>\u001b[0m in \u001b[0;36madd_discounted_reward\u001b[0;34m(steps_episode, gamma)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ml_rewards_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstep_episode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps_episode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_episode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mstep_episode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'discounted_reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_discounted_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rewards_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-a9b8c34e105d>\u001b[0m in \u001b[0;36mget_discounted_reward\u001b[0;34m(t, l_rewards_episode, gamma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_discounted_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rewards_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ml_discounted_reward_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt_prime_reward\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mt_prime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_prime_reward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_rewards_episode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mG_k_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_discounted_reward_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG_k_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a9b8c34e105d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_discounted_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rewards_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ml_discounted_reward_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt_prime_reward\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mt_prime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_prime_reward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_rewards_episode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mG_k_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_discounted_reward_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG_k_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Starting the SARSA learning \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    \n",
    "    (n_episode_steps, done) = (0, False)\n",
    "    evo_episode = {\n",
    "        'episode_sum_reward': 0\n",
    "        , 'evolution_sum_happiness': 0\n",
    "    }\n",
    "    \n",
    "    # Update parameters\n",
    "    epsilon = get_epsilon(episode, init_epsilon)\n",
    "    alpha = get_alpha(episode, init_alpha)\n",
    "    \n",
    "    \n",
    "    # Get episode\n",
    "    steps_episode = []\n",
    "\n",
    "    state1 = env_reset()\n",
    "    evo_episode['evolution_sum_happiness'] += get_happiness(state1)\n",
    "    action1 = epsilon_greedy(Q, state1['state_id'], n_actions, init_epsilon)\n",
    "\n",
    "    while (not done) and (n_episode_steps < nmax_steps):\n",
    "\n",
    "        # Getting the next state \n",
    "        state2, reward1, done, info = env_step(state1, action1)\n",
    "        evo_episode['episode_sum_reward'] += reward1\n",
    "        evo_episode['evolution_sum_happiness'] += get_happiness(state2)\n",
    "        \n",
    "        steps_episode.append({\n",
    "            'state': state1,\n",
    "            'action': action1,\n",
    "            'reward' : reward1})\n",
    "\n",
    "        # Choosing the next action\n",
    "        action2 = epsilon_greedy(Q, state2['state_id'], n_actions, epsilon)\n",
    "\n",
    "        # Updating the respective values \n",
    "        state1 = state2 \n",
    "        action1 = action2\n",
    "        n_episode_steps += 1\n",
    "    \n",
    "    \n",
    "    # Add discounted reward\n",
    "    add_discounted_reward(steps_episode, gamma)\n",
    "\n",
    "    \n",
    "    # Update N and Q\n",
    "    states_already_visited = []\n",
    "    for step_episode in steps_episode:\n",
    "        update_N_MC(N, step_episode, method_monte_carlo, states_already_visited)\n",
    "        update_Q_MC(Q, N, step_episode, method_monte_carlo, states_already_visited)\n",
    "        states_already_visited.append(step_episode['state']['state_id'])\n",
    "\n",
    "\n",
    "    # At the end of learning process \n",
    "    if render_episode:\n",
    "        print('Episode {0}, Score: {1}, Timesteps: {2}, Epsilon: {3}, Alpha: {4}'.format(\n",
    "            episode+1, episode_reward, n_episode_steps, epsilon, alpha))\n",
    "    \n",
    "    evo_training['evo_avg_reward_per_step'].append(evo_episode['episode_sum_reward'] / n_episode_steps)\n",
    "    evo_training['evo_n_steps'].append(n_episode_steps)\n",
    "    evo_training['evo_avg_happiness'].append(evo_episode['evolution_sum_happiness'] / n_episode_steps)\n",
    "    \n",
    "    if ((episode+1) % n_episodes_plot == 0):\n",
    "        \n",
    "        with open('data/interim/{}__Q.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(Q, file)\n",
    "\n",
    "        with open('data/interim/{}__N.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(N, file)\n",
    "\n",
    "        with open('data/interim/{}__evo_training.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(evo_training, file)\n",
    "\n",
    "        #plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "        #plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "        #plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.797334Z",
     "start_time": "2020-05-11T13:15:27.293Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the learning agent - Monte-Carlo - first visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.799403Z",
     "start_time": "2020-05-11T13:15:27.298Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'MC'\n",
    "method_monte_carlo = 'first_visit' # every_visit or first_visit\n",
    "\n",
    "method_name = method + '_' + method_monte_carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.801263Z",
     "start_time": "2020-05-11T13:15:27.303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the Q-matrix \n",
    "Q = init_Q(n_actions, init_Q_type)\n",
    "N = init_N(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.803398Z",
     "start_time": "2020-05-11T13:15:27.306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "(render_episode, render_training) = (False, False)\n",
    "n_episodes_plot = int(np.ceil(n_episodes/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.805128Z",
     "start_time": "2020-05-11T13:15:27.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the reward\n",
    "evo_training = {\n",
    "    'evo_avg_reward_per_step': []\n",
    "    , 'evo_n_steps': []\n",
    "    , 'evo_avg_happiness': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.806890Z",
     "start_time": "2020-05-11T13:15:27.316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Starting the SARSA learning \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    \n",
    "    (n_episode_steps, done) = (0, False)\n",
    "    evo_episode = {\n",
    "        'episode_sum_reward': 0\n",
    "        , 'evolution_sum_happiness': 0\n",
    "    }\n",
    "    \n",
    "    # Update parameters\n",
    "    epsilon = get_epsilon(episode, init_epsilon)\n",
    "    alpha = get_alpha(episode, init_alpha)\n",
    "    \n",
    "    \n",
    "    # Get episode\n",
    "    steps_episode = []\n",
    "\n",
    "    state1 = env_reset()\n",
    "    evo_episode['evolution_sum_happiness'] += get_happiness(state1)\n",
    "    action1 = epsilon_greedy(Q, state1['state_id'], n_actions, init_epsilon)\n",
    "\n",
    "    while (not done) and (n_episode_steps < nmax_steps):\n",
    "\n",
    "        # Getting the next state \n",
    "        state2, reward1, done, info = env_step(state1, action1)\n",
    "        evo_episode['episode_sum_reward'] += reward1\n",
    "        evo_episode['evolution_sum_happiness'] += get_happiness(state2)\n",
    "        \n",
    "        steps_episode.append({\n",
    "            'state': state1,\n",
    "            'action': action1,\n",
    "            'reward' : reward1})\n",
    "\n",
    "        # Choosing the next action\n",
    "        action2 = epsilon_greedy(Q, state2['state_id'], n_actions, epsilon)\n",
    "\n",
    "        # Updating the respective values \n",
    "        state1 = state2 \n",
    "        action1 = action2\n",
    "        n_episode_steps += 1\n",
    "    \n",
    "    \n",
    "    # Add discounted reward\n",
    "    add_discounted_reward(steps_episode, gamma)\n",
    "\n",
    "    \n",
    "    # Update N and Q\n",
    "    states_already_visited = []\n",
    "    for step_episode in steps_episode:\n",
    "        update_N_MC(N, step_episode, method_monte_carlo, states_already_visited)\n",
    "        update_Q_MC(Q, N, step_episode, method_monte_carlo, states_already_visited)\n",
    "        states_already_visited.append(step_episode['state']['state_id'])\n",
    "\n",
    "\n",
    "    # At the end of learning process \n",
    "    if render_episode:\n",
    "        print('Episode {0}, Score: {1}, Timesteps: {2}, Epsilon: {3}, Alpha: {4}'.format(\n",
    "            episode+1, episode_reward, n_episode_steps, epsilon, alpha))\n",
    "    \n",
    "    evo_training['evo_avg_reward_per_step'].append(evo_episode['episode_sum_reward'] / n_episode_steps)\n",
    "    evo_training['evo_n_steps'].append(n_episode_steps)\n",
    "    evo_training['evo_avg_happiness'].append(evo_episode['evolution_sum_happiness'] / n_episode_steps)\n",
    "    \n",
    "    if ((episode+1) % n_episodes_plot == 0):\n",
    "        \n",
    "        with open('data/interim/{}__Q.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(Q, file)\n",
    "\n",
    "        with open('data/interim/{}__N.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(N, file)\n",
    "\n",
    "        with open('data/interim/{}__evo_training.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(evo_training, file)\n",
    "\n",
    "        #plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "        #plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "        #plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.808699Z",
     "start_time": "2020-05-11T13:15:27.321Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the learning agent - SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.810639Z",
     "start_time": "2020-05-11T13:15:27.325Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'SARSA'\n",
    "method_name = method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.812309Z",
     "start_time": "2020-05-11T13:15:27.330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the Q-matrix \n",
    "Q = init_Q(n_actions, init_Q_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.814125Z",
     "start_time": "2020-05-11T13:15:27.334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "(render_episode, render_training) = (False, False)\n",
    "n_episodes_plot = int(np.ceil(n_episodes/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.815761Z",
     "start_time": "2020-05-11T13:15:27.339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the reward\n",
    "evo_training = {\n",
    "    'evo_avg_reward_per_step': []\n",
    "    , 'evo_n_steps': []\n",
    "    , 'evo_avg_happiness': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.817598Z",
     "start_time": "2020-05-11T13:15:27.344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Starting the SARSA learning \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    \n",
    "    (n_episode_steps, done) = (0, False)\n",
    "    evo_episode = {\n",
    "        'episode_sum_reward': 0\n",
    "        , 'evolution_sum_happiness': 0\n",
    "    }\n",
    "    \n",
    "    # Update parameters\n",
    "    epsilon = get_epsilon(episode, init_epsilon)\n",
    "    alpha = get_alpha(episode, init_alpha)\n",
    "    \n",
    "    \n",
    "    # Get episode\n",
    "\n",
    "    state1 = env_reset()\n",
    "    evo_episode['evolution_sum_happiness'] += get_happiness(state1)\n",
    "    action1 = epsilon_greedy(Q, state1['state_id'], n_actions, init_epsilon)\n",
    "\n",
    "    while (not done) and (n_episode_steps < nmax_steps):\n",
    "\n",
    "        # Getting the next state \n",
    "        state2, reward1, done, info = env_step(state1, action1)\n",
    "        evo_episode['episode_sum_reward'] += reward1\n",
    "        evo_episode['evolution_sum_happiness'] += get_happiness(state2)\n",
    "        \n",
    "        # Choosing the next action\n",
    "        action2 = epsilon_greedy(Q, state2['state_id'], n_actions, epsilon)\n",
    "\n",
    "        # Learning the Q-value\n",
    "        Q = update_Q_SARSA(Q,state1['state_id'], action1, reward1, state2['state_id'], action2)\n",
    "        \n",
    "        # Updating the respective values \n",
    "        state1 = state2 \n",
    "        action1 = action2\n",
    "        n_episode_steps += 1\n",
    "    \n",
    "\n",
    "    # At the end of learning process \n",
    "    if render_episode:\n",
    "        print('Episode {0}, Score: {1}, Timesteps: {2}, Epsilon: {3}, Alpha: {4}'.format(\n",
    "            episode+1, episode_reward, n_episode_steps, epsilon, alpha))\n",
    "    \n",
    "    evo_training['evo_avg_reward_per_step'].append(evo_episode['episode_sum_reward'] / n_episode_steps)\n",
    "    evo_training['evo_n_steps'].append(n_episode_steps)\n",
    "    evo_training['evo_avg_happiness'].append(evo_episode['evolution_sum_happiness'] / n_episode_steps)\n",
    "    \n",
    "    if ((episode+1) % n_episodes_plot == 0):\n",
    "        \n",
    "        with open('data/interim/{}__Q.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(Q, file)\n",
    "\n",
    "        with open('data/interim/{}__evo_training.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(evo_training, file)\n",
    "\n",
    "        #plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "        #plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "        #plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.819397Z",
     "start_time": "2020-05-11T13:15:27.348Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the learning agent - Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.821109Z",
     "start_time": "2020-05-11T13:15:27.352Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'Q-Learning'\n",
    "method_name = method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.822780Z",
     "start_time": "2020-05-11T13:15:27.357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the Q-matrix \n",
    "Q = init_Q(n_actions, init_Q_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.824348Z",
     "start_time": "2020-05-11T13:15:27.362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "(render_episode, render_training) = (False, False)\n",
    "n_episodes_plot = int(np.ceil(n_episodes/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.825841Z",
     "start_time": "2020-05-11T13:15:27.366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the reward\n",
    "evo_training = {\n",
    "    'evo_avg_reward_per_step': []\n",
    "    , 'evo_n_steps': []\n",
    "    , 'evo_avg_happiness': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.828330Z",
     "start_time": "2020-05-11T13:15:27.370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Starting the SARSA learning \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    \n",
    "    (n_episode_steps, done) = (0, False)\n",
    "    evo_episode = {\n",
    "        'episode_sum_reward': 0\n",
    "        , 'evolution_sum_happiness': 0\n",
    "    }\n",
    "    \n",
    "    # Update parameters\n",
    "    epsilon = get_epsilon(episode, init_epsilon)\n",
    "    alpha = get_alpha(episode, init_alpha)\n",
    "    \n",
    "    \n",
    "    # Get episode\n",
    "\n",
    "    state1 = env_reset()\n",
    "    evo_episode['evolution_sum_happiness'] += get_happiness(state1)\n",
    "\n",
    "    while (not done) and (n_episode_steps < nmax_steps):\n",
    "        \n",
    "        # Choose an action\n",
    "        action1 = epsilon_greedy(Q, state1['state_id'], n_actions, init_epsilon)\n",
    "\n",
    "        # Getting the next state \n",
    "        state2, reward1, done, info = env_step(state1, action1)\n",
    "        evo_episode['episode_sum_reward'] += reward1\n",
    "        evo_episode['evolution_sum_happiness'] += get_happiness(state2)\n",
    "        \n",
    "\n",
    "        # Q-Learning\n",
    "        # Choosing the next action \n",
    "        action2 = select_best_action(Q[state2['state_id']])\n",
    "        # Learning the Q-value\n",
    "        Q = update_Q_Qlearning(Q, state1['state_id'], action1, reward1, state2['state_id'], action2)\n",
    "        \n",
    "        # Updating the respective values \n",
    "        state1 = state2\n",
    "        n_episode_steps += 1\n",
    "    \n",
    "\n",
    "    # At the end of learning process \n",
    "    if render_episode:\n",
    "        print('Episode {0}, Score: {1}, Timesteps: {2}, Epsilon: {3}, Alpha: {4}'.format(\n",
    "            episode+1, episode_reward, n_episode_steps, epsilon, alpha))\n",
    "    \n",
    "    evo_training['evo_avg_reward_per_step'].append(evo_episode['episode_sum_reward'] / n_episode_steps)\n",
    "    evo_training['evo_n_steps'].append(n_episode_steps)\n",
    "    evo_training['evo_avg_happiness'].append(evo_episode['evolution_sum_happiness'] / n_episode_steps)\n",
    "    \n",
    "    if ((episode+1) % n_episodes_plot == 0):\n",
    "        \n",
    "        with open('data/interim/{}__Q.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(Q, file)\n",
    "\n",
    "        with open('data/interim/{}__evo_training.pkl'.format(method_name), 'wb') as file:\n",
    "            dill.dump(evo_training, file)\n",
    "\n",
    "        #plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "        #plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "        #plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.830400Z",
     "start_time": "2020-05-11T13:15:27.374Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_evolution_reward(evo_training['evo_avg_reward_per_step'], method_name)\n",
    "plot_evolution_steps(evo_training['evo_n_steps'], method_name)\n",
    "plot_evolution_happiness(evo_training['evo_avg_happiness'], method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.832432Z",
     "start_time": "2020-05-11T13:15:27.380Z"
    }
   },
   "outputs": [],
   "source": [
    "list_methods = ['MC_every_visit','MC_first_visit', 'SARSA','Q-Learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.834436Z",
     "start_time": "2020-05-11T13:15:27.383Z"
    }
   },
   "outputs": [],
   "source": [
    "evo_training__evo_avg_reward_per_step = {}\n",
    "evo_training__evo_n_steps = {}\n",
    "evo_training__evo_avg_happiness = {}\n",
    "\n",
    "for method in list_methods:\n",
    "    with open(\"data/interim/{}__evo_training.pkl\".format(method), \"rb\") as input_file:\n",
    "        evo_training = dill.load(input_file)\n",
    "\n",
    "    evo_training__evo_avg_reward_per_step[method] = evo_training['evo_avg_reward_per_step']\n",
    "    evo_training__evo_n_steps[method] = evo_training['evo_n_steps']\n",
    "    evo_training__evo_avg_happiness[method] = evo_training['evo_avg_happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:22:06.836145Z",
     "start_time": "2020-05-11T13:15:27.387Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_comparison_evolution_reward(evo_training__evo_avg_reward_per_step)\n",
    "plot_comparison_evolution_steps(evo_training__evo_n_steps)\n",
    "plot_comparison_evolution_happiness(evo_training__evo_avg_happiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "doggo",
   "language": "python",
   "name": "doggo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
