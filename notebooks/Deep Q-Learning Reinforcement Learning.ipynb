{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "* [Analytics Vidhya - A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python](https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/)\n",
    "* [Analytics Vidhya - Simple Beginnerâ€™s guide to Reinforcement Learning & its implementation](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T07:53:51.774404Z",
     "start_time": "2020-05-19T07:53:51.771123Z"
    }
   },
   "source": [
    "## Install keras-rl library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T07:54:31.985149Z",
     "start_time": "2020-05-19T07:54:31.982604Z"
    }
   },
   "outputs": [],
   "source": [
    "# git clone https://github.com/matthiasplappert/keras-rl.git\n",
    "# cd keras-rl\n",
    "# python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T07:54:08.987545Z",
     "start_time": "2020-05-19T07:54:08.984675Z"
    }
   },
   "source": [
    "## Install dependencies for CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T07:54:32.448419Z",
     "start_time": "2020-05-19T07:54:32.446222Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install h5py\n",
    "# pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T08:16:20.480180Z",
     "start_time": "2020-05-19T08:16:16.725483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments preloaded into gym:\n",
    "* [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/)\n",
    "* [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/)\n",
    "* [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T08:17:35.349560Z",
     "start_time": "2020-05-19T08:17:35.206434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T08:17:37.384391Z",
     "start_time": "2020-05-19T08:17:37.381604Z"
    }
   },
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T07:54:42.813296Z",
     "start_time": "2020-05-19T07:54:39.982797Z"
    }
   },
   "source": [
    "Next, we build a very simple single hidden layer neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T08:17:38.108348Z",
     "start_time": "2020-05-19T08:17:38.028467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T07:54:42.813296Z",
     "start_time": "2020-05-19T07:54:39.982797Z"
    }
   },
   "source": [
    "Next, we configure and compile our agent. We set our policy as Epsilon Greedy and we also set our memory as Sequential Memory because we want to store the result of actions we performed and the rewards we get for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T09:22:18.130424Z",
     "start_time": "2020-05-05T09:22:18.127512Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T09:22:18.138010Z",
     "start_time": "2020-05-05T09:22:18.133937Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(\n",
    "    model=model, nb_actions=n_actions,\n",
    "    memory=memory, nb_steps_warmup=10,\n",
    "    target_model_update=1e-2, policy=policy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T09:22:18.367257Z",
     "start_time": "2020-05-05T09:22:18.142202Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T09:22:45.100861Z",
     "start_time": "2020-05-05T09:22:18.369456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 8:42 - reward: 1.0000WARNING:tensorflow:From /Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/keras_rl-0.4.2-py3.7.egg/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 27s 3ms/step - reward: 1.0000\n",
      "done, took 26.724 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135c12110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now it's time to learn something!\n",
    "# We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T09:22:56.244755Z",
     "start_time": "2020-05-05T09:22:45.103494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 114.000, steps: 114\n",
      "Episode 2: reward: 125.000, steps: 125\n",
      "Episode 3: reward: 145.000, steps: 145\n",
      "Episode 4: reward: 121.000, steps: 121\n",
      "Episode 5: reward: 117.000, steps: 117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136034510>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "doggo",
   "language": "python",
   "name": "doggo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
