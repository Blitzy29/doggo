{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:40.981950Z",
     "start_time": "2020-05-06T11:13:40.975828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Projects/doggo/notebooks\n",
      "/Users/admin/Projects/doggo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "def update_working_directory():\n",
    "    from pathlib import Path\n",
    "    p = Path(os.getcwd()).parents[0]\n",
    "    os.chdir(p)\n",
    "    print(p)\n",
    "update_working_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.343082Z",
     "start_time": "2020-05-06T11:13:40.986725Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.349667Z",
     "start_time": "2020-05-06T11:13:41.345397Z"
    }
   },
   "outputs": [],
   "source": [
    "decimals_state = 2\n",
    "def get_state_id(dog_state):\n",
    "    return '{:01.4f}_{:01.4f}_{:01.4f}_{}'.format(\n",
    "        dog_state['food'], dog_state['fat'], dog_state['affection'], dog_state['can_action_be_taken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.357743Z",
     "start_time": "2020-05-06T11:13:41.353369Z"
    }
   },
   "outputs": [],
   "source": [
    "def env_reset():\n",
    "    \n",
    "    dog_state = {\n",
    "        'food': 0.5,\n",
    "        'fat': 0,\n",
    "        'affection': 0.5,\n",
    "        'last_action_taken': 0,\n",
    "        'minutes_since_last_action': 0,\n",
    "        'can_action_be_taken': True\n",
    "        }\n",
    "    \n",
    "    dog_state['state_id'] = get_state_id(dog_state)\n",
    "    \n",
    "    return dog_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.373575Z",
     "start_time": "2020-05-06T11:13:41.361649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': 0.5,\n",
       " 'fat': 0,\n",
       " 'affection': 0.5,\n",
       " 'last_action_taken': 0,\n",
       " 'minutes_since_last_action': 0,\n",
       " 'can_action_be_taken': True,\n",
       " 'state_id': '0.5000_0.0000_0.5000_True'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.380263Z",
     "start_time": "2020-05-06T11:13:41.376049Z"
    }
   },
   "outputs": [],
   "source": [
    "# n_states = env.observation_space.n\n",
    "# n_states = 11*11*11\n",
    "# n_actions = env.action_space.n\n",
    "n_actions= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.388691Z",
     "start_time": "2020-05-06T11:13:41.382683Z"
    }
   },
   "outputs": [],
   "source": [
    "WALKING_TIME = 15\n",
    "EATING_TIME = 1\n",
    "PLAYING_TIME = 4\n",
    "\n",
    "food_consumption_rate = 1.0 / (30 * 3600)\n",
    "affection_consumption_rate = 1.0 / (50 * 3600)\n",
    "walking_fat_converge_rate = 0.2\n",
    "walking_affection_converge_rate = 0.4\n",
    "playing_fat_converge_rate = 0.1\n",
    "playing_affection_converge_rate = 0.20\n",
    "eating_food_increase = 0.6\n",
    "eating_fat_increase = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.397158Z",
     "start_time": "2020-05-06T11:13:41.392371Z"
    }
   },
   "outputs": [],
   "source": [
    "def round_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "def round_down(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.416442Z",
     "start_time": "2020-05-06T11:13:41.403866Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_decreasing_rate(value: float, rate: float) -> float:\n",
    "    \"\"\"\n",
    "    Apply a decreasing rate to a value\n",
    "    :param value: current value\n",
    "    :param rate: per second\n",
    "    :return: updated value\n",
    "    \"\"\"\n",
    "    return value - (60 * rate)\n",
    "\n",
    "def converge(value: float, target: float, ratio: float) -> float:\n",
    "    diff: float = (target - value) * ratio\n",
    "    return value + diff\n",
    "\n",
    "\n",
    "def update_food(dog_state):\n",
    "    update_food = apply_decreasing_rate(dog_state['food'], food_consumption_rate)\n",
    "    return round_down(max(0.0, update_food), decimals=decimals_state)\n",
    "\n",
    "def update_fat(dog_state):\n",
    "    update_fat = dog_state['fat']\n",
    "    return update_fat\n",
    "\n",
    "def update_affection(dog_state):\n",
    "    update_affection = apply_decreasing_rate(dog_state['affection'], affection_consumption_rate)\n",
    "    return round_down(max(0.0, update_affection), decimals=decimals_state)\n",
    "\n",
    "\n",
    "def update_if_walking(dog_state):\n",
    "    update_fat = round_down(converge(dog_state['fat'], 0.0, walking_fat_converge_rate), decimals=decimals_state)\n",
    "    update_affection = round_up(converge(dog_state['affection'], 1.0, walking_affection_converge_rate), decimals=decimals_state)\n",
    "    return (update_fat, update_affection)\n",
    "\n",
    "def update_if_feeding(dog_state):\n",
    "    update_food = round_up(min(dog_state['food'] + eating_food_increase, 1.0), decimals=decimals_state)\n",
    "    update_fat = round_up(min(dog_state['fat'] + eating_fat_increase, 1.0), decimals=decimals_state)\n",
    "    return (update_food, update_fat)\n",
    "\n",
    "def update_if_playing(dog_state):\n",
    "    update_fat = round_down(converge(dog_state['fat'], 0.0, playing_fat_converge_rate), decimals=decimals_state)\n",
    "    update_affection = round_up(converge(dog_state['affection'], 1.0, playing_affection_converge_rate), decimals=decimals_state)\n",
    "    return (update_fat, update_affection)\n",
    "\n",
    "\n",
    "def get_happiness(dog_state):\n",
    "    happiness = min(dog_state['food'], 1.0 - dog_state['fat'], dog_state['affection'])\n",
    "    return happiness\n",
    "\n",
    "\n",
    "def update_done(dog_state):\n",
    "    happiness = get_happiness(dog_state)\n",
    "    return happiness <= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.431463Z",
     "start_time": "2020-05-06T11:13:41.420524Z"
    }
   },
   "outputs": [],
   "source": [
    "# state2, reward1, done, info = env.step(action1)\n",
    "def env_step(state1, action):\n",
    "    \n",
    "    state2 = state1.copy()\n",
    "    reward_penalty = 0\n",
    "    \n",
    "    # Affect of time\n",
    "    state2['food'] = update_food(state2)\n",
    "    state2['fat'] = update_fat(state2)\n",
    "    state2['affection'] = update_affection(state2)\n",
    "    state2['minutes_since_last_action'] += 1 \n",
    "    \n",
    "    # Applying action\n",
    "    if action != 0:\n",
    "        if state2['can_action_be_taken']:\n",
    "            reward_penalty += 0.1\n",
    "            state2['can_action_be_taken'] = False\n",
    "            state2['minutes_since_last_action'] = 0\n",
    "            state2['last_action_taken'] = action\n",
    "        else:\n",
    "            reward_penalty += 0.5\n",
    "\n",
    "    # Affect of actions\n",
    "    if (state2['last_action_taken'] == 1) & (state2['minutes_since_last_action'] == WALKING_TIME):\n",
    "        state2['fat'], state2['affection'] = update_if_walking(state2)\n",
    "        state2['can_action_be_taken'] = True\n",
    "\n",
    "    if (state2['last_action_taken'] == 2) & (state2['minutes_since_last_action'] == EATING_TIME):\n",
    "        state2['food'], state2['fat'] = update_if_feeding(state2)\n",
    "        state2['can_action_be_taken'] = True\n",
    "\n",
    "    if (state2['last_action_taken'] == 3) & (state2['minutes_since_last_action'] == PLAYING_TIME):\n",
    "        state2['fat'], state2['affection'] = update_if_playing(state2)\n",
    "        state2['can_action_be_taken'] = True\n",
    "                    \n",
    "    done = update_done(state2)\n",
    "    if done:\n",
    "        reward = -10\n",
    "    else:\n",
    "        reward = min(state2['food'], 1.0 - state2['fat'], state2['affection']) - reward_penalty\n",
    "    \n",
    "    info = None\n",
    "    \n",
    "    state2['state_id'] = get_state_id(state2)\n",
    "    \n",
    "    return (state2, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.437768Z",
     "start_time": "2020-05-06T11:13:41.433419Z"
    }
   },
   "outputs": [],
   "source": [
    "def env_render(dog_state, action, Q):\n",
    "    print(dog_state)\n",
    "    print(action)\n",
    "    print(Q[dog_state['state_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining utility functions to be used in the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.445269Z",
     "start_time": "2020-05-06T11:13:41.439901Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_Q(n_actions, init_Q_type=\"ones\"):\n",
    "    \"\"\"\n",
    "    @param n_actions the number of actions\n",
    "    @param type random, ones or zeros for the initialization\n",
    "    \"\"\"\n",
    "    if init_Q_type == \"ones\":\n",
    "        default_Q_values = np.ones(n_actions)\n",
    "    elif init_Q_type == \"random\":\n",
    "        default_Q_values = np.random.random(n_actions)\n",
    "    elif init_Q_type == \"zeros\":\n",
    "        default_Q_values = np.zeros(n_actions)\n",
    "    \n",
    "    def get_default_Q_values():\n",
    "        return default_Q_values\n",
    "\n",
    "    return defaultdict(get_default_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.450567Z",
     "start_time": "2020-05-06T11:13:41.447424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy generator\n",
    "rng = np.random.default_rng()  # Create a default Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.456709Z",
     "start_time": "2020-05-06T11:13:41.452943Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_best_action(Q_state):\n",
    "    winner = np.argwhere(Q_state == np.amax(Q_state))\n",
    "    winner_list = winner.flatten().tolist()\n",
    "    action = random.choice(winner_list)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.463415Z",
     "start_time": "2020-05-06T11:13:41.458757Z"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state_id, n_actions, epsilon):\n",
    "    \"\"\"\n",
    "    @param Q Q values {state, action} -> value\n",
    "    @param epsilon for exploration\n",
    "    @param n_actions number of actions\n",
    "    @param state state at time t\n",
    "    \"\"\"\n",
    "    if rng.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        action = select_best_action(Q[state_id])\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Q-matrice (state-action value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.471628Z",
     "start_time": "2020-05-06T11:13:41.465549Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to learn the Q-value  - Is it temporal-difference?\n",
    "def update(state1_id, action1, reward1, state2_id, action2, expected=False):\n",
    "    \n",
    "    previous_Q_value_state1 = Q[state1_id].copy()\n",
    "    \n",
    "    predict = Q[state1_id][action1] \n",
    "    \n",
    "    target = reward1 + gamma * Q[state2_id][action2] \n",
    "    if expected:\n",
    "        expected_value = np.mean(Q[state2_id])\n",
    "        target = reward1 + gamma * expected_value\n",
    "    \n",
    "    new_Q_value = Q[state1_id][action1] + alpha * (target - predict)\n",
    "    previous_Q_value_state1[action1] = new_Q_value\n",
    "    \n",
    "    Q[state1_id] = previous_Q_value_state1\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon $\\epsilon$ - Exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.477048Z",
     "start_time": "2020-05-06T11:13:41.473277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exploration rate\n",
    "\n",
    "def get_epsilon(episode, init_epsilon, divisor=25):\n",
    "    \n",
    "    n_epsilon = init_epsilon/(episode/10000+1)\n",
    "    # n_epsilon = min(1, 1.0 - math.log10((episode + 1) / divisor))\n",
    "    \n",
    "    return n_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha $\\alpha$ - Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.482503Z",
     "start_time": "2020-05-06T11:13:41.479133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "\n",
    "def get_alpha(episode, init_alpha, divisor=25):\n",
    "    \n",
    "    n_alpha = init_alpha/(episode/10000+1)\n",
    "    # n_alpha = min(1.0, 1.0 - math.log10((episode + 1) / divisor))\n",
    "    \n",
    "    return n_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots Reward / Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.489065Z",
     "start_time": "2020-05-06T11:13:41.485047Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.701943Z",
     "start_time": "2020-05-06T11:13:41.491168Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evolution_reward(evolution_reward):\n",
    "    \n",
    "    n_moving_points = int(np.ceil(len(evolution_reward)/100))\n",
    "    y = running_mean(evolution_reward,n_moving_points)\n",
    "    x = range(len(y))\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.title('Evolution of Reward over time (smoothed over window size 100)')\n",
    "    plt.xlabel('Episode') # will add a label “Year” to your x-axis\n",
    "    plt.ylabel('Episode Reward (Smoothed)') # will add a label “Population” to your y-axis\n",
    "    plt.xticks() # set the numbers on the x-axis to be 1, 2, 3, 4, 5. We can also pass and labels as a second argument. For, example, if we use this code plt.xticks([1, 2, 3, 4, 5], [\"1M\", \"2M\", \"3M\", \"4M\", \"5M\"]), it will set the labels 1M, 2M, 3M, 4M, 5M on the x-axis.\n",
    "    plt.yticks() # - works the same as plt.xticks(), but for the y-axis.\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.709918Z",
     "start_time": "2020-05-06T11:13:41.704135Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evolution_steps(evolution_steps):\n",
    "    \n",
    "    n_moving_points = int(np.ceil(len(evolution_steps)/100))\n",
    "    y = running_mean(evolution_steps,n_moving_points)\n",
    "    x = range(len(y))\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.title('Episode length over time (smoothed over window size 100)')\n",
    "    plt.xlabel('Episode') # will add a label “Year” to your x-axis\n",
    "    plt.ylabel('Episode Length (Smoothed)') # will add a label “Population” to your y-axis\n",
    "    plt.xticks() # set the numbers on the x-axis to be 1, 2, 3, 4, 5. We can also pass and labels as a second argument. For, example, if we use this code plt.xticks([1, 2, 3, 4, 5], [\"1M\", \"2M\", \"3M\", \"4M\", \"5M\"]), it will set the labels 1M, 2M, 3M, 4M, 5M on the x-axis.\n",
    "    plt.yticks() # - works the same as plt.xticks(), but for the y-axis.\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.717841Z",
     "start_time": "2020-05-06T11:13:41.712261Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evolution_happiness(evolution_happiness_all):\n",
    "    \n",
    "    n_moving_points = int(np.ceil(len(evolution_happiness_all)/100))\n",
    "    y = running_mean(evolution_happiness_all,n_moving_points)\n",
    "    x = range(len(y))\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.title('Happiness over time (smoothed)')\n",
    "    plt.xlabel('Episode') # will add a label “Year” to your x-axis\n",
    "    plt.ylabel('Happiness (Smoothed)') # will add a label “Population” to your y-axis\n",
    "    plt.xticks() # set the numbers on the x-axis to be 1, 2, 3, 4, 5. We can also pass and labels as a second argument. For, example, if we use this code plt.xticks([1, 2, 3, 4, 5], [\"1M\", \"2M\", \"3M\", \"4M\", \"5M\"]), it will set the labels 1M, 2M, 3M, 4M, 5M on the x-axis.\n",
    "    plt.yticks() # - works the same as plt.xticks(), but for the y-axis.\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.724029Z",
     "start_time": "2020-05-06T11:13:41.720132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the different parameters \n",
    "init_epsilon = 1 # trade-off exploration/exploitation - better if decreasing\n",
    "init_alpha = 0.5 # learning rate, better if decreasing\n",
    "\n",
    "# Specific to environment\n",
    "gamma = 0.95 # discount for future rewards (also called decay factor)\n",
    "# n_states = env.observation_space.n # useless\n",
    "n_actions = 4\n",
    "\n",
    "# Episodes\n",
    "n_episodes = 1000000\n",
    "nmax_steps = 60*24*30 # maximum steps per episode\n",
    "\n",
    "# Initializing the Q-matrix \n",
    "Q = init_Q(n_actions, init_Q_type=\"zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.733096Z",
     "start_time": "2020-05-06T11:13:41.730180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "(render_episode, render_training) = (False, False)\n",
    "n_episodes_plot = int(np.ceil(n_episodes/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T11:13:41.739403Z",
     "start_time": "2020-05-06T11:13:41.736696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the reward\n",
    "evolution_reward = []\n",
    "evolution_steps = []\n",
    "evolution_happiness_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T12:25:07.836391Z",
     "start_time": "2020-05-11T12:25:07.719114Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff2a1d73027e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Starting the SARSA learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_episode_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# Starting the SARSA learning \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    \n",
    "    n_episode_steps = 0\n",
    "    episode_reward = 0\n",
    "    evolution_happiness = []\n",
    "    done = False\n",
    "    \n",
    "    state1 = env_reset()\n",
    "    evolution_happiness.append(get_happiness(state1))\n",
    "    action1 = epsilon_greedy(Q, state1['state_id'], n_actions, init_epsilon)\n",
    "    \n",
    "    while (not done) and (n_episode_steps < nmax_steps):\n",
    "    \n",
    "        # Update parameters\n",
    "        epsilon = get_epsilon(episode, init_epsilon)\n",
    "        alpha = get_alpha(episode, init_alpha)\n",
    "    \n",
    "        # Visualizing the training\n",
    "        if render_training:\n",
    "            env_render(state1, action1, Q)\n",
    "    \n",
    "        # Getting the next state \n",
    "        state2, reward1, done, info = env_step(state1, action1)\n",
    "        episode_reward += reward1\n",
    "        evolution_happiness.append(get_happiness(state2))\n",
    "    \n",
    "        # Choosing the next action\n",
    "        action2 = epsilon_greedy(Q, state2['state_id'], n_actions, epsilon)\n",
    "\n",
    "        # Learning the Q-value\n",
    "        Q = update(state1['state_id'], action1, reward1, state2['state_id'], action2)\n",
    "\n",
    "        # Updating the respective values \n",
    "        state1 = state2 \n",
    "        action1 = action2\n",
    "        n_episode_steps += 1\n",
    "        \n",
    "    # At the end of learning process \n",
    "    if render_episode:\n",
    "        print('Episode {0}, Score: {1}, Timesteps: {2}, Epsilon: {3}, Alpha: {4}'.format(\n",
    "            episode+1, episode_reward, n_episode_steps, epsilon, alpha))\n",
    "    \n",
    "    evolution_reward.append(episode_reward)\n",
    "    evolution_steps.append(n_episode_steps)\n",
    "    evolution_happiness_all.append(np.mean(evolution_happiness))\n",
    "    \n",
    "    if ((episode+1) % n_episodes_plot == 0):\n",
    "        plot_evolution_reward(evolution_reward)\n",
    "        plot_evolution_steps(evolution_steps)\n",
    "        plot_evolution_happiness(evolution_happiness_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.829660Z",
     "start_time": "2020-05-06T11:13:41.059Z"
    }
   },
   "outputs": [],
   "source": [
    "pct_state_visited = len(Q)/(101*101*101)*100\n",
    "print(pct_state_visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.831227Z",
     "start_time": "2020-05-06T11:13:41.062Z"
    }
   },
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.833146Z",
     "start_time": "2020-05-06T11:13:41.064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the performance \n",
    "print (\"Performance : \", sum(evolution_reward)/n_episodes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of Reward overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.835018Z",
     "start_time": "2020-05-06T11:13:41.067Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_evolution_reward(evolution_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.837138Z",
     "start_time": "2020-05-06T11:13:41.068Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_evolution_steps(evolution_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation through episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T13:43:06.114222Z",
     "start_time": "2020-05-05T13:43:06.111453Z"
    }
   },
   "source": [
    "### One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.838634Z",
     "start_time": "2020-05-06T11:13:41.071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "nmax_steps = 20000\n",
    "\n",
    "n_episode_steps = 0\n",
    "evolution_episode_reward = []\n",
    "done = False\n",
    "\n",
    "# Start episode and get initial observation\n",
    "state = env_reset()\n",
    "\n",
    "while (not done) and (n_episode_steps < nmax_steps):\n",
    "\n",
    "    # Get an action (0:Left, 1:Down, 2:Right, 3:Up)\n",
    "    action = select_best_action(Q_state = Q[state['state_id']])\n",
    "\n",
    "    # Perform a step\n",
    "    state, reward, done, info = env_step(state, action)\n",
    "\n",
    "    # Update score\n",
    "    evolution_episode_reward.append(reward)\n",
    "    n_episode_steps += 1\n",
    "\n",
    "print('Test Episode, Score: {0}, Timesteps: {1}'.format(\n",
    "    sum(evolution_episode_reward)/n_episode_steps, n_episode_steps))\n",
    "\n",
    "plot_evolution_reward(evolution_episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.840334Z",
     "start_time": "2020-05-06T11:13:41.074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "episodes = 100\n",
    "nmax_steps = 20000\n",
    "total_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.841876Z",
     "start_time": "2020-05-06T11:13:41.076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop episodes\n",
    "for episode in range(episodes):\n",
    "\n",
    "    n_episode_steps = 0\n",
    "    evolution_episode_reward = []\n",
    "    evolution_dog_happines = []\n",
    "    done = False\n",
    "    \n",
    "    # Start episode and get initial observation\n",
    "    state = env_reset()\n",
    "    evolution_dog_happines.append(get_happiness(state))\n",
    "    \n",
    "    while (not done) and (n_episode_steps < nmax_steps):\n",
    "\n",
    "        # Get an action (0:Left, 1:Down, 2:Right, 3:Up)\n",
    "        action = select_best_action(Q_state = Q[state['state_id']])\n",
    "        \n",
    "        # Perform a step\n",
    "        state, reward, done, info = env_step(state, action)\n",
    "        evolution_dog_happines.append(get_happiness(state))\n",
    "        \n",
    "        # Update score\n",
    "        evolution_episode_reward.append(reward)\n",
    "        n_episode_steps += 1\n",
    "\n",
    "    print('Episode {0}, Score: {1}, Timesteps: {2}'.format(\n",
    "        episode+1, sum(evolution_episode_reward)/n_episode_steps, n_episode_steps))\n",
    "    \n",
    "    plot_evolution_reward(evolution_dog_happines)\n",
    "    total_reward.append(sum(evolution_dog_happines)/n_episode_steps)\n",
    "\n",
    "# Print the score\n",
    "print('--- Evaluation ---')\n",
    "print ('Score: {0} +/- {1}'.format(np.mean(total_reward), statistics.stdev(total_reward)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.843134Z",
     "start_time": "2020-05-06T11:13:41.079Z"
    }
   },
   "outputs": [],
   "source": [
    "state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.844785Z",
     "start_time": "2020-05-06T11:13:41.082Z"
    }
   },
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.846237Z",
     "start_time": "2020-05-06T11:13:41.084Z"
    }
   },
   "outputs": [],
   "source": [
    "state0 = env_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.847927Z",
     "start_time": "2020-05-06T11:13:41.087Z"
    }
   },
   "outputs": [],
   "source": [
    "state1, reward, _, _  = env_step(state0, action = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.849405Z",
     "start_time": "2020-05-06T11:13:41.089Z"
    }
   },
   "outputs": [],
   "source": [
    "state1, reward, _, _  = env_step(state1, action = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.850884Z",
     "start_time": "2020-05-06T11:13:41.091Z"
    }
   },
   "outputs": [],
   "source": [
    "state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.852774Z",
     "start_time": "2020-05-06T11:13:41.093Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    state1, reward, done, _  = env_step(state1, action = 2)\n",
    "    print(state1)\n",
    "    print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.854317Z",
     "start_time": "2020-05-06T11:13:41.094Z"
    }
   },
   "outputs": [],
   "source": [
    "state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:38:09.856048Z",
     "start_time": "2020-05-06T11:13:41.097Z"
    }
   },
   "outputs": [],
   "source": [
    "get_happiness(state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:53:53.171630Z",
     "start_time": "2020-05-06T12:53:53.162723Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dill'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6811cc16f03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dill'"
     ]
    }
   ],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:57:10.497743Z",
     "start_time": "2020-05-06T12:57:10.495085Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T12:57:19.551109Z",
     "start_time": "2020-05-06T12:57:19.541380Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'init_Q.<locals>.get_default_Q_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d9780e71746e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'init_Q.<locals>.get_default_Q_values'"
     ]
    }
   ],
   "source": [
    "pickle.dumps(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "doggo",
   "language": "python",
   "name": "doggo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
