{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "* [Adventures in Machine Learning - Reinforcement learning tutorial using Python and Keras](https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the network is the one-hot encoded state vector. For instance, the vector which corresponds to state 1 is [0, 1, 0, 0, 0] and state 3 is [0, 0, 0, 1, 0]. In this case, a hidden layer of 10 nodes with sigmoid activation will be used. The output layer is a linear activated set of two nodes, corresponding to the two Q values assigned to each state to represent the two possible actions. Linear activation means that the output depends only on the linear summation of the inputs and the weights, with no additional function applied to that summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building this network is easy in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T09:05:13.765982Z",
     "start_time": "2020-05-19T09:05:10.808740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense\n",
    "# from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T09:05:13.913689Z",
     "start_time": "2020-05-19T09:05:13.778853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/admin/Projects/doggo/.venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T09:05:32.092025Z",
     "start_time": "2020-05-19T09:05:32.077093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (1, 10)                   60        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 2)                    22        \n",
      "=================================================================\n",
      "Total params: 82\n",
      "Trainable params: 82\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the model is created using the Keras Sequential API. Then an input layer is added which takes inputs corresponding to the one-hot encoded state vectors. Then the sigmoid activated hidden layer with 10 nodes is added, followed by the linear activated output layer which will yield the Q values for each action. Finally the model is compiled using a mean-squared error loss function (to correspond with the loss function defined previously) with the Adam optimizer being used in its default Keras state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this model in the training environment, the following code is run which is similar to the previous -greedy Q learning methodology with an explicit Q table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now execute the q learning\n",
    "y = 0.95\n",
    "eps = 0.5\n",
    "decay_factor = 0.999\n",
    "\n",
    "r_avg_list = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    eps *= decay_factor\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "    \n",
    "    done = False\n",
    "    r_sum = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if np.random.random() < eps:\n",
    "            a = np.random.randint(0, 2)\n",
    "        else:\n",
    "            a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "        \n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "        target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "        target_vec[a] = target\n",
    "        model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "        \n",
    "        s = new_s\n",
    "        r_sum += r\n",
    "    \n",
    "    r_avg_list.append(r_sum / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first major difference in the Keras implementation is the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if np.random.random() < eps:\n",
    "#     a = np.random.randint(0, 2)\n",
    "# else:\n",
    "#     a = np.argmax(model.predict(np.identity(5)[s:s + 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first condition in the if statement is the implementation of the $\\epsilon$-greedy action selection policy that has been discussed already. The second condition uses the Keras model to produce the two Q values â€“ one for each possible state. It does this by calling the model.predict() function. Here the numpy identity function is used, with vector slicing, to produce the one-hot encoding of the current state s. The standard numpy argmax function is used to select the action with the highest Q value returned from the Keras model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second major difference is the following four lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "# target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "# target_vec[a] = target\n",
    "# model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line sets the target as the Q learning updating rule that has been previously presented. It is the reward r plus the discounted maximum of the predicted Q values for the new state, new_s. This is the value that we want the Keras model to learn to predict for state s and action a i.e. Q(s,a). However, our Keras model has an output for each of the two actions â€“ we donâ€™t want to alter the value for the other action, only the action a which has been chosen. So on the next line, target_vec is created which extracts both predicted Q values for state s. On the following line, only the Q value corresponding to the action a is changed to target â€“ the other actionâ€™s Q value is left untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final line is where the Keras model is updated in a single training step. The first argument is the current state â€“ i.e. the one-hot encoded input to the model. The second is our target vector which is reshaped to make it have the required dimensions of (1, 2). The third argument tells the fit function that we only want to train for a single iteration and finally the verbose flag simply tells Keras not to print out the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this training over 1000 game episodes reveals the following average reward for each step in the game:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, the average reward per step in the game increases over each game episode, showing that the Keras model is learning well (if a little slowly).\n",
    "\n",
    "We can also run the following code to get an output of the Q values for each of the states â€“ this is basically getting the Keras model to reproduce our explicit Q table that was generated in previous methods:\n",
    "\n",
    "State 0 â€“ action [[62.734287 61.350456]]\n",
    "\n",
    "State 1 â€“ action [[66.317955 62.27209 ]]\n",
    "\n",
    "State 2 â€“ action [[70.82501 63.262383]]\n",
    "\n",
    "State 3 â€“ action [[76.63797 64.75874]]\n",
    "\n",
    "State 4 â€“ action [[84.51073 66.499725]]\n",
    "\n",
    "This output looks sensible â€“ we can see that the Q values for each state will favor choosing action 0 (moving forward) to shoot for those big, repeated rewards in state 4. Intuitively, this seems like the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "doggo",
   "language": "python",
   "name": "doggo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
